{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade pip\n",
    "#!pip install --upgrade tensorflow\n",
    "#!pip install --upgrade theano\n",
    "#!pip install --upgrade keras\n",
    "#!pip install --upgrade keras-text\n",
    "#!pip install --upgrade tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>change_after_3_days</th>\n",
       "      <th>change_after_2_days</th>\n",
       "      <th>change_after_1_days</th>\n",
       "      <th>binary_sentiment_after_3_days</th>\n",
       "      <th>sentiment_after_3_days</th>\n",
       "      <th>binary_sentiment_after_2_days</th>\n",
       "      <th>sentiment_after_2_days</th>\n",
       "      <th>binary_sentiment_after_1_days</th>\n",
       "      <th>sentiment_after_1_days</th>\n",
       "      <th>title_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-05-31</th>\n",
       "      <td>24415.839844</td>\n",
       "      <td>429740000</td>\n",
       "      <td>5.54548</td>\n",
       "      <td>-0.175886</td>\n",
       "      <td>2.518083</td>\n",
       "      <td>pos</td>\n",
       "      <td>negative</td>\n",
       "      <td>neg</td>\n",
       "      <td>positive</td>\n",
       "      <td>pos</td>\n",
       "      <td>negative</td>\n",
       "      <td>racial bias in bail decisions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-31</th>\n",
       "      <td>24415.839844</td>\n",
       "      <td>429740000</td>\n",
       "      <td>5.54548</td>\n",
       "      <td>-0.175886</td>\n",
       "      <td>2.518083</td>\n",
       "      <td>pos</td>\n",
       "      <td>negative</td>\n",
       "      <td>neg</td>\n",
       "      <td>positive</td>\n",
       "      <td>pos</td>\n",
       "      <td>negative</td>\n",
       "      <td>to hit canada with steel aluminum tariffs at m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-31</th>\n",
       "      <td>24415.839844</td>\n",
       "      <td>429740000</td>\n",
       "      <td>5.54548</td>\n",
       "      <td>-0.175886</td>\n",
       "      <td>2.518083</td>\n",
       "      <td>pos</td>\n",
       "      <td>negative</td>\n",
       "      <td>neg</td>\n",
       "      <td>positive</td>\n",
       "      <td>pos</td>\n",
       "      <td>negative</td>\n",
       "      <td>walmart perk for workers go to college for day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-31</th>\n",
       "      <td>24415.839844</td>\n",
       "      <td>429740000</td>\n",
       "      <td>5.54548</td>\n",
       "      <td>-0.175886</td>\n",
       "      <td>2.518083</td>\n",
       "      <td>pos</td>\n",
       "      <td>negative</td>\n",
       "      <td>neg</td>\n",
       "      <td>positive</td>\n",
       "      <td>pos</td>\n",
       "      <td>negative</td>\n",
       "      <td>why china looming debt problems will not stop ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-31</th>\n",
       "      <td>24415.839844</td>\n",
       "      <td>429740000</td>\n",
       "      <td>5.54548</td>\n",
       "      <td>-0.175886</td>\n",
       "      <td>2.518083</td>\n",
       "      <td>pos</td>\n",
       "      <td>negative</td>\n",
       "      <td>neg</td>\n",
       "      <td>positive</td>\n",
       "      <td>pos</td>\n",
       "      <td>negative</td>\n",
       "      <td>italy financial crisis explained</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Close     Volume  change_after_3_days  change_after_2_days  \\\n",
       "2018-05-31  24415.839844  429740000              5.54548            -0.175886   \n",
       "2018-05-31  24415.839844  429740000              5.54548            -0.175886   \n",
       "2018-05-31  24415.839844  429740000              5.54548            -0.175886   \n",
       "2018-05-31  24415.839844  429740000              5.54548            -0.175886   \n",
       "2018-05-31  24415.839844  429740000              5.54548            -0.175886   \n",
       "\n",
       "            change_after_1_days binary_sentiment_after_3_days  \\\n",
       "2018-05-31             2.518083                           pos   \n",
       "2018-05-31             2.518083                           pos   \n",
       "2018-05-31             2.518083                           pos   \n",
       "2018-05-31             2.518083                           pos   \n",
       "2018-05-31             2.518083                           pos   \n",
       "\n",
       "           sentiment_after_3_days binary_sentiment_after_2_days  \\\n",
       "2018-05-31               negative                           neg   \n",
       "2018-05-31               negative                           neg   \n",
       "2018-05-31               negative                           neg   \n",
       "2018-05-31               negative                           neg   \n",
       "2018-05-31               negative                           neg   \n",
       "\n",
       "           sentiment_after_2_days binary_sentiment_after_1_days  \\\n",
       "2018-05-31               positive                           pos   \n",
       "2018-05-31               positive                           pos   \n",
       "2018-05-31               positive                           pos   \n",
       "2018-05-31               positive                           pos   \n",
       "2018-05-31               positive                           pos   \n",
       "\n",
       "           sentiment_after_1_days  \\\n",
       "2018-05-31               negative   \n",
       "2018-05-31               negative   \n",
       "2018-05-31               negative   \n",
       "2018-05-31               negative   \n",
       "2018-05-31               negative   \n",
       "\n",
       "                                                  title_clean  \n",
       "2018-05-31                      racial bias in bail decisions  \n",
       "2018-05-31  to hit canada with steel aluminum tariffs at m...  \n",
       "2018-05-31     walmart perk for workers go to college for day  \n",
       "2018-05-31  why china looming debt problems will not stop ...  \n",
       "2018-05-31                   italy financial crisis explained  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import google.datalab.storage as storage\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "\n",
    "mybucket = storage.Bucket('bharti_patel')\n",
    "data_csv = mybucket.object('merged2_data.csv')\n",
    "\n",
    "uri = data_csv.uri\n",
    "%gcs read --object $uri --variable data\n",
    "df = pd.read_csv(BytesIO(data), index_col=0, parse_dates=True, infer_datetime_format=True)\n",
    "df.head()\n",
    "\n",
    "\n",
    "mybucket = storage.Bucket('bharti_patel')\n",
    "data2_csv = mybucket.object('merged_data.csv')\n",
    "\n",
    "uri = data2_csv.uri\n",
    "%gcs read --object $uri --variable data2\n",
    "df2 = pd.read_csv(BytesIO(data2), index_col=0, parse_dates=True, infer_datetime_format=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>sentiment_after_3_days</th>\n",
       "      <th>binary_sentiment_after_3_days</th>\n",
       "      <th>title_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-03-31</td>\n",
       "      <td>negative</td>\n",
       "      <td>pos</td>\n",
       "      <td>[silver gold the last american hero jfk, resou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-04-01</td>\n",
       "      <td>positive</td>\n",
       "      <td>neg</td>\n",
       "      <td>[copper pipes now worth more than some homes, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008-04-02</td>\n",
       "      <td>positive</td>\n",
       "      <td>neg</td>\n",
       "      <td>[bush caused the financial market meltdown]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008-04-04</td>\n",
       "      <td>negative</td>\n",
       "      <td>pos</td>\n",
       "      <td>[trucker view of the us economy, la boy whirlp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008-04-11</td>\n",
       "      <td>negative</td>\n",
       "      <td>pos</td>\n",
       "      <td>[dollar falls below yuan for first time since ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     level_0 sentiment_after_3_days binary_sentiment_after_3_days  \\\n",
       "0 2008-03-31               negative                           pos   \n",
       "1 2008-04-01               positive                           neg   \n",
       "2 2008-04-02               positive                           neg   \n",
       "3 2008-04-04               negative                           pos   \n",
       "4 2008-04-11               negative                           pos   \n",
       "\n",
       "                                         title_clean  \n",
       "0  [silver gold the last american hero jfk, resou...  \n",
       "1  [copper pipes now worth more than some homes, ...  \n",
       "2        [bush caused the financial market meltdown]  \n",
       "3  [trucker view of the us economy, la boy whirlp...  \n",
       "4  [dollar falls below yuan for first time since ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.groupby([ pd.Grouper(freq='D'), 'sentiment_after_3_days', 'binary_sentiment_after_3_days'])['title_clean'].apply(list).reset_index()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = df.title_clean\n",
    "y = df.sentiment_after_3_days\n",
    "z = df.binary_sentiment_after_3_days\n",
    "u = df2.title_clean\n",
    "\n",
    "MAX_SENT_LENGTH = 1000\n",
    "MAX_SENTS = 50\n",
    "MAX_NB_WORDS = 100000\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py3env/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow\n",
      "Change Keras Backend to tensorflow\n",
      "tensorflow\n"
     ]
    }
   ],
   "source": [
    "from keras import backend; print(backend._BACKEND)\n",
    "from keras import backend as K\n",
    "import importlib\n",
    "import os\n",
    "def set_keras_backend(backend):\n",
    "    if K.backend() != backend:\n",
    "        os.environ['KERAS_BACKEND'] = backend\n",
    "        importlib.reload(K)\n",
    "        assert K.backend() == backend\n",
    "print (\"Change Keras Backend to tensorflow\")        \n",
    "set_keras_backend(\"tensorflow\")  \n",
    "from keras import backend; print(backend._BACKEND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# define baseline model\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D, BatchNormalization, GRU\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Reshape\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer , text_to_word_sequence\n",
    "from keras.layers import Input, Dense, concatenate, Activation\n",
    "from keras.models import Model\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "#import keras_metrics\n",
    "import tensorflow as tf\n",
    "from keras import initializers\n",
    "from keras import backend as K\n",
    "#import theano as th\n",
    "from keras.utils import multi_gpu_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def as_keras_metric(method):\n",
    "    import functools\n",
    "    from keras import backend as K\n",
    "    import tensorflow as tf\n",
    "    @functools.wraps(method)\n",
    "    def wrapper(self, args, **kwargs):\n",
    "        \"\"\" Wrapper for turning tensorflow metrics into keras metrics \"\"\"\n",
    "        value, update_op = method(self, args, **kwargs)\n",
    "        K.get_session().run(tf.local_variables_initializer())\n",
    "        with tf.control_dependencies([update_op]):\n",
    "            value = tf.identity(value)\n",
    "        return value\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "precision = as_keras_metric(tf.metrics.precision)\n",
    "recall = as_keras_metric(tf.metrics.recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py3env/lib/python3.5/site-packages/keras_preprocessing/text.py:175: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(u)\n",
    "sequences = tokenizer.texts_to_sequences(u)\n",
    "\n",
    "vocab_size = len(u)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16859"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'microfinance'\n",
    "tokenizer.word_index[word]\n",
    "#x = x.str.replace('microfinace', 'microfinance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1310, 50, 1000)\n",
      "Total 61210 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#from keras_text.processing import SentenceWordTokenizer\n",
    "\n",
    "data = np.zeros((vocab_size, MAX_SENTS, MAX_SENT_LENGTH), dtype='float64')\n",
    "\n",
    "print (data.shape)\n",
    "  \n",
    "\n",
    "\n",
    "for i, sentences in enumerate(x):\n",
    "    for j, sent in enumerate(sentences):\n",
    "        #if j< MAX_SENTS:\n",
    "         wordTokens = text_to_word_sequence(sent)\n",
    "         k=0\n",
    "         for _, word in enumerate(wordTokens):\n",
    "              #if k<MAX_SENT_LENGTH and tokenizer.word_index[word]<MAX_NB_WORDS:\n",
    "               #print (word)\n",
    "               #print(tokenizer.word_index[word])\n",
    "               if word == 'microfinace':\n",
    "                  word = 'microfinance'\n",
    "                  data[i,j,k] = tokenizer.word_index[word]\n",
    "               k=k+1                    \n",
    "               \n",
    "word_index = tokenizer.word_index\n",
    "print('Total %s unique tokens.' % len(word_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of seq_label tensor: (1310,)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "enc = LabelEncoder()\n",
    "onehot = OneHotEncoder(sparse=False)\n",
    "seq_label = enc.fit_transform(y)\n",
    "#seq_label = seq_label.reshape(len(seq_label), 1)\n",
    "#seq_label_hot = onehot.fit_transform(seq_label)\n",
    "print('Shape of seq_label tensor:', seq_label.shape)\n",
    "print(seq_label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('./glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "# find the embedding vector dimensions size from the most common word in english: 'the'\n",
    "# http://www.dictionary.com/e/commonwords/\n",
    "embedding_size = len(embeddings_index['the'])\n",
    "print (embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.random.random((len(word_index) + 1, embedding_size))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            embedding_size,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SENT_LENGTH,\n",
    "                            trainable=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embedding matrix: (61211, 100)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of embedding matrix:', embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sequences shape: (1049, 50, 1000)\n",
      "Training labels shape: (1049, 3)\n",
      "Test sequences shape: (131, 50, 1000)\n",
      "Test labels shape: (131, 3)\n",
      "Validation sequences shape: (130, 50, 1000)\n",
      "Validation labels shape: (130, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "train_pct = 0.8\n",
    "test_pct = 1 - train_pct\n",
    "validation_pct = 0.5\n",
    "SEED = 2000\n",
    "seq_label = to_categorical(seq_label)\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "seq_label = seq_label[indices]\n",
    "nb_test_validation_samples = int(test_pct * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_test_validation_samples]\n",
    "y_train = seq_label[:-nb_test_validation_samples]\n",
    "x_val_test = data[-nb_test_validation_samples:]\n",
    "y_val_test = seq_label[-nb_test_validation_samples:]\n",
    "\n",
    "nb_test_samples = int(validation_pct * x_val_test.shape[0])\n",
    "\n",
    "x_test = x_val_test[:-nb_test_samples]\n",
    "y_test = y_val_test[:-nb_test_samples]\n",
    "x_val = x_val_test[-nb_test_samples:]\n",
    "y_val = y_val_test[-nb_test_samples:]\n",
    "\n",
    "\n",
    "print(\"Training sequences shape:\", x_train.shape)\n",
    "print(\"Training labels shape:\",y_train.shape)\n",
    "print(\"Test sequences shape:\", x_test.shape)\n",
    "print(\"Test labels shape:\", y_test.shape)\n",
    "print(\"Validation sequences shape:\", x_val.shape)\n",
    "print(\"Validation labels shape:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - Hierachical LSTM\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 50, 1000)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 50, 200)           6281900   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 200)               240800    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 603       \n",
      "=================================================================\n",
      "Total params: 6,523,303\n",
      "Trainable params: 6,523,303\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sentence_input)\n",
    "l_lstm = Bidirectional(LSTM(100))(embedded_sequences)\n",
    "sentEncoder = Model(sentence_input, l_lstm)\n",
    "\n",
    "input = Input(shape=(MAX_SENTS,MAX_SENT_LENGTH), dtype='int32')\n",
    "encoder = TimeDistributed(sentEncoder)(input)\n",
    "l_lstm_sent = Bidirectional(LSTM(100))(encoder)\n",
    "preds = Dense(3, activation='softmax')(l_lstm_sent)\n",
    "model = Model(input, preds)\n",
    "#parallel_model = multi_gpu_model(model, gpus=None)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(\"model fitting - Hierachical LSTM\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc362fc4080>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "\n",
    "model.fit(x_test, y_test, epochs=50, verbose=0,validation_data=(x_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "#loss, accuracy, precision, recall = model.evaluate(x_test, y_test, verbose=0)\n",
    "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.580153\n"
     ]
    }
   ],
   "source": [
    "#print('Accuracy: %f, precision: %f, recall: %f, f1_score: %f' % ((accuracy), (precision), (recall), (2 * precision * recall / (precision + recall))))\n",
    "print('Accuracy: %f' % (accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 50, 1000)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_18 (TimeDistri (None, 50, 1000, 100 6121100     input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_19 (TimeDistri (None, 50, 999, 100) 20100       time_distributed_18[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_21 (TimeDistri (None, 50, 998, 100) 30100       time_distributed_18[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_23 (TimeDistri (None, 50, 997, 100) 40100       time_distributed_18[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_20 (TimeDistri (None, 50, 100)      0           time_distributed_19[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_22 (TimeDistri (None, 50, 100)      0           time_distributed_21[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_24 (TimeDistri (None, 50, 100)      0           time_distributed_23[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 50, 300)      0           time_distributed_20[0][0]        \n",
      "                                                                 time_distributed_22[0][0]        \n",
      "                                                                 time_distributed_24[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_9 (Bidirectional) (None, 200)          320800      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 3)            603         bidirectional_9[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 6,532,803\n",
      "Trainable params: 6,532,803\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input = Input(shape=(MAX_SENTS,MAX_SENT_LENGTH), dtype='float64')\n",
    "embedded_sequences = embedding_layer(input)\n",
    "\n",
    "encoder = TimeDistributed(embedding_layer)(input)\n",
    "\n",
    "bigram_branch = TimeDistributed(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1))(encoder)\n",
    "bigram_branch = TimeDistributed(GlobalMaxPooling1D())(bigram_branch)\n",
    "trigram_branch = TimeDistributed(Conv1D(filters=100, kernel_size=3, padding='valid', activation='relu', strides=1))(encoder)\n",
    "trigram_branch = TimeDistributed(GlobalMaxPooling1D())(trigram_branch)\n",
    "fourgram_branch = TimeDistributed(Conv1D(filters=100, kernel_size=4, padding='valid', activation='relu', strides=1))(encoder)\n",
    "fourgram_branch = TimeDistributed(GlobalMaxPooling1D())(fourgram_branch)\n",
    "merged = concatenate([bigram_branch, trigram_branch, fourgram_branch])\n",
    "\n",
    "\n",
    "LSTM_out = Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2))(merged)\n",
    "\n",
    "output = (Dense(3, activation='sigmoid'))(LSTM_out)\n",
    "\n",
    "model2 = Model(inputs=[input], outputs=[output])\n",
    "\n",
    "model2.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "          metrics=['accuracy',keras_metrics.precision(), keras_metrics.recall()])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f60ca092128>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "\n",
    "model2.fit(x_test, y_test, epochs=50, verbose=0,validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy, precision, recall = model2.evaluate(x_test, y_test, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.496183, precision: 0.450382, recall: 0.900763, f1_score: 0.600509\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: %f, precision: %f, recall: %f, f1_score: %f' % ((accuracy), (precision), (recall), (2 * precision * recall / (precision + recall))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
